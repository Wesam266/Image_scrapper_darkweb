{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import traceback\n",
    "import socks\n",
    "import socket\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "from shutil import copyfileobj\n",
    "from urllib.parse import urlparse, ParseResult\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "\n",
    "\n",
    "# Results path\n",
    "scrapping_path = '/home/wesam/datasets/Scraping_Result_22_01_2017/'\n",
    "\n",
    "# Error path\n",
    "error_scraping_path = '/home/wesam/datasets/Scraping_Result_22_01_2017/errors.txt'\n",
    "\n",
    "# The HTML dir\n",
    "data_set_path = '/home/wesam/datasets/Onion_Dataset/{0}/{0}.html'\n",
    "\n",
    "\n",
    "ext_list = ['jpg', 'png', 'gif', 'jpeg']\n",
    "\n",
    "def main():\n",
    "    with open('white_list.txt') as f:\n",
    "        white_list = f.readlines()\n",
    "\n",
    "\n",
    "    f_error = open(error_scraping_path, 'a', encoding='utf-8')\n",
    "\n",
    "    for file in white_list:\n",
    "        print (file)\n",
    "        links_list = []\n",
    "        try:\n",
    "            with open (data_set_path.format(file.strip()),'r', encoding='utf-8' , errors='ignore') as ins:\n",
    "                txt= ' '.join(ins.readlines())    \n",
    "        except Exception as e:\n",
    "            print ('exception in ', file, e)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(txt, 'html.parser')\n",
    "\n",
    "        for link in soup.findAll(\"a\"):\n",
    "            try:\n",
    "                href = link['href']\n",
    "                if href.lower()[-3:] in ext_list:\n",
    "                    links_list.append (href)\n",
    "            except Exception:\n",
    "                continue \n",
    "\n",
    "        for link in soup.findAll(\"img\"):\n",
    "            try:            \n",
    "                href = link['src']\n",
    "                if href.lower()[-3:] in ext_list:\n",
    "                    links_list.append (href)\n",
    "            except:\n",
    "                continue\n",
    "        links_list =  list(set(links_list))\n",
    "        if len(links_list)>0:\n",
    "            if not os.path.exists(scrapping_path + file ):\n",
    "                os.makedirs(scrapping_path + file)\n",
    "                #print ( len(links_list),file  )\n",
    "                for imgUrl in links_list:\n",
    "                    try:   \n",
    "                        if urlparse(imgUrl).netloc=='':\n",
    "                            imgUrl =urljoin(file, imgUrl)\n",
    "                        imgName= scrapping_path + file + '/'+ imgUrl.split('/')[-1]\n",
    "                        if not os.path.exists(imgName):\n",
    "                            with urlopen(imgUrl, timeout=1) as in_stream, open(imgName, 'wb') as out_file:\n",
    "                                copyfileobj(in_stream, out_file)                    \n",
    "                    except Exception:\n",
    "                        f_error.write('{0}\\t{1}\\t{2}\\n'.format(file, imgName, imgUrl))\n",
    "                        continue\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "def create_connection(address, timeout=None, source_address=None):\n",
    "    sock = socks.socksocket()\n",
    "    sock.connect(address)\n",
    "    return sock\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(scrapping_path):\n",
    "        os.mkdir(scrapping_path)\n",
    "        \n",
    "    tor_server = str('127.0.0.1')\n",
    "    tor_port = int('9050')\n",
    "    socks.setdefaultproxy(socks.PROXY_TYPE_SOCKS5, tor_server, tor_port)\n",
    "\n",
    "    # patch the socket module\n",
    "    socket.socket = socks.socksocket\n",
    "    socket.create_connection = create_connection\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
